<div class="solid-line"></div>
<div id="sectionHeader">
    <h5>Dynamic Characterization of Information Actors</h5>
    <p>
        We will discuss our approach to characterizing a dynamic social/campaign process. We will first
        provide a theoretical framework to characterize a social campaign and describe various theoretical
        constructs. Then, we will discuss operationalization of these constructs
        (computationally, and for social media) to characterize the dynamics of a campaign. The
        characterization framework is depicted in Figure 1.

        <div>
            <img src="../assets/images/p1_diagram_oie_tactic.png" alt=disinformationkilldiagram
                width="80%">
        </div>
        <p>
            <b>Figure 7.</b>  Framework for Dynamic Characterization of Information Actors – Producers and Consumers. . 
        </p>
    </p>
</div>

<br>
<div class="collapsible" id="video-signal">
    <button type="button" class="collapsible_button">
        <h6>Dynamic Characterization of Information Producers
        </h6>
    </button>

    <div class="collapsible-content">
        <p>We leverage the diffusion of innovations (DOI) theory to show how users adopt into social
            media campaigns. The psychological and social behaviors that are understood from the DOI
            model can be applied to social networks to categorize social media users into adoption
            categories
            and to use diffusion curves to determine if a campaign has reached its tipping point (see Figure
            2). We demonstrate this application of DOI theory to social networks in (Spann, et al., 2022),
            where we show how the spread of misinformation follows a trajectory pattern similar to the
            adoption of new technologies. The framework in this proposal considers these two approaches to
            understanding the diffusion of information in a connective action campaign. This view can be
            used to help shape collective identity and understand whether a collective response is a
            function
            of aggregation effects or specific interaction conditions within a group. This research also
            consider how online social networks are used for the purpose of social media campaigns. The
            sociotechnical theories of affordance and interdependence amongst users are applied to
            understand how individual user participation impacts the rate of adoption into an information
            campaign. The interactions between users in a connective action campaign can provide insights to
            help determine how future actors will react as information propagates through the network.
            Leonardi distinguished between these types of actions by labeling them as individualized,
            shared, and collective affordances (Leonardi, 2013). The interdependencies created between users
            creates a production function that allows us to measure the diffusion of information in a cyber
            campaign. The initial observations from our analysis support a mathematical characterization for
            connective action campaigns and show how affordances can be used to identify the inter-user
            message strategy (Spann, et al., 2020).
            <br>
        </p>
        <br>
        <img src="../assets/images/doi_modelofCampaignCycleTrajectory.png" alt="commenter-diagram" ,
            width="80%">
        <br><br><b>Figure 2. </b>Diffusion of Innovation (DOI) model and stages for user adoption (left).
        Model of s-
        curve campaign cycle trajectory (right).
        <p><br>
            We leverage the diffusion of innovations (DOI) theory to examine the trajectory of the overall
            campaign as it evolves. The second derivative test is then applied to identify changes in the
            slope
            of the s-curve. The second derivative test is a method in multivariable calculus used to
            determine
            if a critical point of a function is a local minimum, maximum or saddle point. This empirical
            approach provides an initial quantitative method to find the points at which the slope of the
            curves shifts from the accelerating stage to the point at which the campaign reaches maximum
            diffusion or critical mass, and when the slope flattens out or enters the decelerating stage.
            Since
            DOI theory has been applied to various disciplines, a key benefit to this approach is that the
            s-
            curves are platform independent. Understanding these processes provides a foundation for
            predicting when an information campaign reaches critical mass.
        </p>
        <p>
            In fully adopted information campaigns such as Figure 3, we observe an s-shaped phenomenon.
            Applying the concept of adoption stages from DOI, we see there is an initial growth stage,
            accelerating stage, and then a decelerating stage. The COVID-19 misinformation campaign
            #BillGatesVirus (Figure 3, left) provides a classic example of the s-curve feature showing the
            full life cycle of how the campaign started slowly, experienced rapid growth, and then slows
            toward the end of the campaign. Another COVID-19 misinformation campaign, viz.,
            #NoVaccineForMe (Figure 3, right) depicts multiple/cascaded s-shaped adoption, suggesting
            continuous monitoring of misinformation campaigns for future action. We use DOI theory as a
            guiding principle and apply the concept to suggest new categories for information actors that
            align with the cascaded adoption phenomenon, viz., initiators, amplifiers, and sustainers.
        </p>
        <br>
        <img src="../assets/images/fullAdoptionCycleForBillGate.png" alt="commenter-diagram" , width="80%">
        <br><br><b>Figure 3. </b>Information Actor Characterization based on s-Shape Connective Action
        Process
        Dynamics. Full Adoption Cycle for #BillGatesVirus (left) and Cascaded Adoption Cycle for
        #NoVaccineForMe (right).
    </div>
</div>

<br>
<div class="collapsible" id="video-signal">
    <button type="button" class="collapsible_button">
        <h6>Dynamic Characterization of Information Consumers
        </h6>
    </button>

    <div class="collapsible-content">
        <p>Applying a mathematical model to understand the trends and dynamics of the spread of
            misinformation and opposing perspectives on online social networks can help develop
            techniques to quickly identify and mitigate the spread. The approach presented here attempts to
            understand the influence that the propagation of misinformation on social media has on the
            beliefs and behaviors of information consumers and characterize them based on their actions.
            two most common epidemiological models, viz., SIR and SEIZ are evaluated. <br>
        </p>
        <br>
        <p><br>
            SIR Model: The SIR model is an oft-used fundamental epidemiological model. The SIR model
            partitions members of a population into three compartments: Susceptible (S), Infected (I), and
            Recovered (R) (Figure 4). In the SIR model, people in the Infected compartment are those
            determined to have contracted a specific infection or disease and can spread the infection to
            other
            members of the population. The Susceptible compartment includes members of the population
            considered at risk of contracting the infection from the Infected members. The Recovered
            compartment consists of members immune from contracting the infection or who have died from
            the infection. By allocating new definitions to these terms, the SIR model can be adapted to
            include the spread of information on online social network (OSN), treating "information" as the
            "infection", and OSN users as the population. In this revised SIR model, the use of a specific
            hashtag or a video can be considered as the information and the "infection". Therefore, a user
            who has posted using the specific hashtag or posted a specific video can be considered Infected.
            Similarly, a user can be considered Susceptible if they follow an infected user but have not yet
            posted using that specific hashtag themselves. Finally, a user can be considered Recovered if
            they have not made additional posts containing the specific hashtag or the video within a
            certain
            defined time frame.
            <br><br>
            SEIZ Model: Basic epidemiological models such as SIR are limited by accounting for only one
            possible path from the Susceptible compartment, which is to enter the Infected compartment. In
            the case of information as the infection, however, users in the Susceptible compartment have
            additional paths possible for transition. They can transition to the Infected compartment by
            deciding to post the information using the specific hashtag or the video. They can also decide
            not
            to post but continue to follow the infected user. They therefore move into an Exposed
            compartment, meaning that they are still at risk. In addition, some users may need time to
            decide
            if they believe the information and should spread the hashtags or videos related to them. Users
            in
            the Susceptible compartment can indicate that they are decidedly skeptical of the information.
            Finally, some users in the Susceptible compartment show no indication of having been exposed
            to the information. None of these additional possibilities are considered via the basic SIR
            epidemiological model but are accounted for in the more robust SEIZ model. We applied both
            the SIR and SEIZ model to our data to illustrate the comparative ability to model the
            propagation
            of information on OSN.
            <br><br>
            When applying the SEIZ epidemiological model to Twitter data with the objective of analyzing
            the propagation of information, the composition of the compartments (Figure 5) can be
            considered as follows: Infected (I) consists of the users who have posted a video. Susceptible
            (S)
            consists of users who follow the Infected users. Exposed (E) consists of users exposed to the
            video (or a hashtag-specific tweet) and, after some time, also commented on the video. Skeptic
            (Z) consists of users who have been exposed to the video but have decided not to engage
        </p>
        <br>
        <img src="../assets/images/sirModel.png" alt="commenter-diagram" , width="20%">
        <p>
            <b>&ensp;Figure 3. </b>SIR Model
        </p>
        <br>
        <img src="../assets/images/seizModel.png" alt="commenter-diagram" , width="20%">
        <p>
            <b>&ensp;Figure 4. </b>SEIZ Model
        </p>
        <br>
        <img src="../assets/images/sirAntilock.png" alt="commenter-diagram" , width="60%">
        <p>
            <b>&ensp;Figure 5. </b>SIR model fit for Anti-lockdown narrative (error = 26.4%).
        </p>
        <br>
        <img src="../assets/images/seizAntilock.png" alt="commenter-diagram" , width="60%">
        <p>
            <b>&ensp;Figure 6. </b>SEIZ model fit for Anti-lockdown narrative (error = 6.9%).
        </p>
        <p>
            We applied SIR and SEIZ epidemiological models to various datasets with COVID-19 context
            (e.g., face masks, lockdowns, vaccine, 5G, Bill Gates). Figures 7 and 7 show comparative
            analysis of SIR and SEIZ models for diffusion of the anti-lockdown narrative. The error reported
            for SIR and SEIZ models is 26.4% and 6.9%, respectively. In this analysis, we applied
            epidemiological models to a relatively small dataset (Maleki, et al., 2021) and our findings
            showed that the SEIZ model can efficiently fit the spreaders of some misinformation and
            legitimate narratives more than the other epidemiological models. For example, in cases where
            the campaign cycle adheres to the s-shape adoption characteristics more strictly (e.g.,
            #Nofacemask and #Wearafacemask), the error for the fitted ‘I (Infected)’ compartment is
            relatively low compared to the datasets that do not conform strictly to the s-shaped adoption
            characteristics (e.g., #lockdownkills) (Maleki et al., 2021; 2021a).
        </p>
    </div>
</div>

<br>
<div class="collapsible" id="toxicitypropagation">
    <button type="button" class="collapsible_button">
        <h6>Dynamic Characterization of Information Actors based on Toxicity
            Propagation
        </h6>
    </button>

    <div class="collapsible-content">
        <p>Discourse toxicity has a profound impact on community dynamics. Extreme toxic levels may
            splinter communities (DiCicco et al., 2022). Adversarial information actors adopt this tactic to
            reshape communities of interest. Toxicity’s impact on community dynamics can be explained
            using the contagion phenomenon of toxicity (Obadimu at al., 2019; 2019a; 2021) (Maleki et al.,
            (2022) on YouTube and Twitter. Information actors engage with toxic discourse in different and
            dynamic capacities. Due to contagious property of toxicity, epidemiological modeling could help
            in information actor characterization.

            <br><br>
            We evaluated five different forms of toxicity (i.e., obscene, threat, insult, identity attack,
            and sexually explicit) between the comments posted on pro and anti-NATO channels on YouTube
            (Obadimu et al. 2019). The analysis demonstrated that comments on pro-NATO channels are less
            toxic than those on the anti-NATO channels. We analyzed toxic ideas related to COVID-19 and
            users who spread them on YouTube (Obadimu et al., 2021). We showed the concepts of “contagion”
            and “polarization” on a social network. We further showed how the contagion of toxicity in a
            social network can be contained in order to improve the overall health of a social network
            (Obadimu et al., 2019a). These simulations rely on the removal of highly toxic nodes in the
            network that serve as “bridges” or “information brokers” within the network that perpetuate the
            contagion and polarization effect. For example, in the network depicted in Figure 8, each node
            represents a user, and the edge represents a video in common. An initial set of 2,453 users
            commented on the same video with at least one other user. Several commenters served as
            “gatekeepers” by forming bridges between the smaller clusters enabling them to spread toxic
            information quickly. Gatekeepers can select, channel, shape, manipulate, and delete information
            from the network solely based on their position. If any of these commenters are removed, the
            network structure will disintegrate.
            <br>
            <div>
                <img src="../assets/images/commenterNetwork.png" alt="commenter-diagram" , width="80%">

            </div>
            <br><br><b>Figure 8. </b>Co-commenter shared video network. Colors denote toxicity scores from
            lowest 0
            (blue) to highest 1 (red).

            <br>
            Figure 9 shows a commenter-reply network for the same dataset. This was created as a directed
            graph to demonstrate the flow of conversation and understand how toxicity spreads from nodes
            to nodes and becomes contagious. Each edge represents a reply relation between a commenter
            and the replier with the arrow pointing towards the replier. One can observe a high degree of
            segregation/polarization among commenters based on toxicity. The red or blue colored nodes are
            collocated most of the time suggesting that the replies to toxic comments had a similar level of
            toxicity. In other words, highly toxic commenters are more likely to form groups with
            commenters with high toxicity and low toxic commenters tend to cluster with commenters who
            are less toxic. Even though the network components are quite isolated from one another, various
            connections existed within these individual components. Figures 8 and 9 both show the
            polarization effect based on toxicity.

            <br>
            <img src="../assets/images/commenter-reply-network.png" alt="commenter-diagram" , width="80%">
            <br><br><b>Figure 9. </b>Commenter-reply directed network clusters depicting segregation among
            commenters based on toxicity. Colors denote toxicity scores from lowest 0 (blue) to highest 1
            (red).
            <br><br>
            We applied a mathematical epidemiological theory-based model to explain how toxicity
            propagates on online social networks (OSNs). The model considers three categories of
            information actors based on their role in toxicity propagation, viz., Susceptible (S), Toxic
            (T),
            Recovered (R), and again Susceptible (S), or STRS model (pronounced as “stars”). The STRS
            model was evaluated on COVID-19 narratives to characterize information actors based on
            toxicity propagation. We collected data for different narratives that best cover a broad range
            of
            topics related to COVID-19 during the entirety of 2020. These topics included three main topics:
            lockdowns, masks, and vaccines. The narratives were chosen after a qualitative OSN analysis
            found frequent and common narratives during the pandemic. Our dataset contains original posts,
            as well as shares, and replies. There was no language restriction as the toxicity computation
            includes non-English languages. The list of identified narratives and their respective number of
            posts is reported in Table 1. Table 1 also includes the error metric relative to our
            experiments,
            discussed in more detail next.

            <br><br>
            The cumulative sums of posts for different datasets are shown in Figure 10.
            When viewed on a cumulative scale, we can see that the overall posting
            frequency identified as having the potential for propagating toxicity
            regarding masks is more prolific than for those regarding lockdowns and
            vaccines. Interestingly, however, there are two considerable spikes within
            the lockdown dataset: late May 2020, and late October 2020. Of further note,
            an s-shaped curve can be seen within the cumulative toxicity spread for both
            masks and vaccines. Within the cumulative toxicity spread related to
            lockdown, however, there are two apparent s-shaped curves. These
            consecutive s-curves depict a cascaded spread of toxicity demonstrating a
            transition of information actors from susceptible to toxic to recovered to
            susceptible again. Our STRS model is able to capture this dynamic
            characterization of information actors.

            <b>Table 3.</b>COVID-19 narratives, number of posts, and STRS model error.
            <div>
                <img src="../assets/images/p3_strs_model_error.png" alt=disinformationkilldiagram
                    width="80%">
                    <div>
                        <img src="../assets/images/p3_cum_sum_narrative.png" alt=disinformationkilldiagram
                            width="80%">
                    </div>
            </div>
    
            <b>Figure 10.</b>Cumulative sum of posts for narratives related to lockdown, mask, and vaccine shown in Table 1.

            <br><br>
            We analyzed the toxicity scores for each of these components (S, T, R, S) for
            each post in our four datasets (shown in Table 1). The average overall
            toxicity scores for anti-mask, anti-vaccines, anti-lockdown, and COVID
            disbelief are 0.61, 0.51, 0.55, and 0.42, respectively. The average toxicity
            score for the combined datasets is 0.52. These scores confirm that our
            collected datasets contain toxic discourse. Content defined as “toxic” is
            considered to be a unit of text input assigned a toxicity score of 0.5 or
            greater. Figure 11 shows the percentage of toxic posts within the toxicity,
            obscene, and insult categories for each dataset. One of the interesting
            findings is that 99.9 percent of the posts related to lockdown have a score
            greater than 0.5 for obscene, which may indicate a high level of anger or
            otherwise negative sentiment from users regarding the topic of mandatory
            lockdowns due to COVID-19.

            <div>
                <img src="../assets/images/p3_to.png" alt=disinformationkilldiagram
                    width="80%">
            </div>
    </p>
    <p>
      
    </p>
    <p>
        <br>
        <br>
        We then fit the number of toxic people (users who posted toxic content) in
        each 24-hour time interval as the Toxic (T) compartment in the STRS model.
        The STRS model concepts are covered in more depth in (Obadimu et al.,
        2021).
        <div>
            <img src="../assets/images/p3_strs_anti_mask.png" alt=disinformationkilldiagram
                width="80%">
        </div>
    </p>
    <p>
        <b>&ensp;Figure 12. </b>STRS model fit for the anti-mask narrative.
        <b>&ensp;Figure 12. </b>STRS model fit for the anti-mask narrative.
        <div>
            <img src="../assets/images/p3_strs_anti_covid.png" alt=disinformationkilldiagram
                width="80%">
        </div>
    </p>

    <p>
        <b>Figure 13.</b> STRS model fit for the anti-COVID (COVID disbelief) narrative.

    </p>
    </p>
      

            <br><br>
            The key findings of this study include:
            <br>&#x2022;Based on the error metrics calculated for the four datasets used in our experiments
            (summarized in Table 2), the STRS model accurately predicts the Toxic (Infected) compartment
            when applied to the spread of toxicity on OSNs.
            <br>&#x2022;The trends observed in the extracted datasets could be divided into three broad
            categories: (a) s-shaped curves (Figure 12), (b) cascaded s-shaped curves, and (c) non s-shaped
            curve (Figure 13).
            <br>&#x2022;
            <br><br>Although the overall performance of the STRS model was promising and satisfactory to
            make predictions in the case of s-shaped diffusion and in cases of straight (or close to
            straight) lines, fitting cascaded s-shaped curves produced the highest level of error. Modeling
            of cascaded s-shaped diffusion requires further examination.
            <br><br>According to our analysis, three out of our four toxic datasets with toxicity scores of
            0.5 or greater were estimated by the STRS model with the lowest possible error. These three
            datasets are: COVID-19, vaccines, and masks. The lockdown dataset presents a higher error of fit
            due to its cascaded s-shaped diffusion trend.
            out of our four toxic datasets with toxicity
            scores of 0.5 or greater were estimated by the STRS model with the lowest
            possible error. These three datasets are: COVID-19, vaccines, and masks.
            The lockdown dataset presents a higher error of fit due to its cascaded s-
            shaped diffusion trend.

            <br><br>
            Table 2 shows the data elements and analytical methodology taxonomy needed to operationalize
            the framework presented above to characterize information actors dynamically on multimedia
            rich OIE. A content taxonomy of the various information modalities is included here.
            <br><br>
            <b>Table 2</b> Data Elements and Analytical Methodologies needed to Operationalize the
            Connective
            Action Framework to Characterize Dynamic Social Campaign Process on Multimedia rich OIE.

            <br><br>
            <img src="../assets/images/tableConnectActionFramework.png" alt="commenter-diagram" ,
                width="80%">

            <br><br>
            A comprehensive characterization of the rich data environment of multimedia OIE and
            characterization of multimodal multimethod analytical methodologies for multimedia OIE are
            depicted in Figures 14 and 15, respectively.
        </p>
    </div>
</div>


<div id="my_tree" style="width: 100%;height:1000px;border:1px solid rgb(241, 228, 228);"></div>
<br>
<div class="collapsible" id="high-level-diagram">
    <button type="button" class="collapsible_button">
        <h5> Futher Reading
        </h5>
    </button>

    <div class="collapsible-content">
        <p>
            Leonardi, P. M. (2013). When Does Technology Use Enable Network Change in Organizations? A
            Comparative Study of Feature Use and Shared Affordances, MIS Quarterly (37:3), pp.749-775.
            <br> Maleki, M., Arani, M., Buchholz, E., Mead, E., & Agarwal, N. (2021). Applying an
            Epidemiological Model to Evaluate the Propagation of Misinformation and Legitimate
            COVID-19-Related Information on Twitter. International Conference on Social Computing,
            Behavioral-Cultural Modeling and Prediction and Behavior Representation in Modeling and
            Simulation, 23–34. https://doi.org/10.1007/978-3-030-80387-2_3
            <br> Maleki, M., Mead, E., Arani, M., & Agarwal, N. (2021a). Using an Epidemiological Model to
            Study the Spread of Misinformation during the Black Lives Matter Movement.
            https://doi.org/10.48550/arxiv.2103.12191
            <br> Maleki, M., Mead, E. L., Agarwal, N., Arani, M., Mead, E., & Kready, J. (2022). Applying an
            Epidemiological Model to Evaluate the Propagation of Toxicity related to COVID-19 on Twitter.
            Hawaii International Conference on System Sciences (HICSS), 1–10.
            https://doi.org/10.24251/HICSS.2022.401
            <br> Obadimu, A., Khaund, T., Mead, E., Marcoux, T., & Agarwal, N. (2021). Developing a
            socio-computational approach to examine toxicity propagation and regulation in COVID-19
            discourse on YouTube. Information Processing and Management 58, 5 (2021), 102660.
            https://doi.org/10.1016/j.ipm.2021.102660.
            <br>Obadimu, A., Mead, E., & Agarwal, N. (2019). Identifying Latent Toxic Features on YouTube
            Using Non-negative Matrix Factorization.

            <br>Obadimu, A., Mead, E., Hussain, M. N., & Agarwal, N. (2019a). Identifying Toxicity Within
            YouTube Video Comment. In R. Thomson, H. Bisgin, C. Dancy, & A. Hyder (Eds.), Social, Cultural,
            and Behavioral Modeling (pp. 214–223). Springer International Publishing.

            <br>Spann, B., Agarwal, N., Johnson, S., and Mead, E., (2020). Modeling Protester Orchestration
            through Connective Action: A COVID-19 Lockdown Protest Case Study. 2020 International Conference
            on Social Computing, Behavioral-Cultural Modeling & Prediction, and Behavior Representation in
            Modeling and Simulation (SBP-BRiMS 2020), October 18-21, 2020, Washington D.C.

            <br>Spann, B., Mead, E., Maleki, M., Agarwal, N., and Williams, T. (2022). Applying Diffusion of
            Innovations Theory to Social Networks to Understand the Stages of Adoption in Connective Action
            Campaigns. Online Social Networks and Media, Elsevier. Vol. 28, March. DOI:
            10.1016/j.osnem.2022.100201.
        </p>
    </div>
</div>