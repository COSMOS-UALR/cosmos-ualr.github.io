<!--01 Review of Existing Framework-->
<div id="reviewExisting">
    <br>
    <h5>Review of Existing Frameworks</h5>
    <p>
        This section provides a quick summary of the existing frameworks to characterize and communicate
        disinformation, and incidents of influence operations across online platforms. Although this section
        includes some of the most prominent and widely mentioned frameworks, it is non-exhaustive, as
        efforts by governments, intergovernmental organizations, and the industry to adapt to the evolving
        threat environment continue.
    </p>
</div>

<br>
<div class="collapsible" id="framework-for-actor">
    <button type="button" class="collapsible_button">
        <h6>Frameworks for Actor Motivation, Documentation, and Inter-Institutional Communication
        </h6>
    </button>

    <div class="collapsible-content">
        <div>
            <p>
                Prominently after Russian Federation's illegal annexation of Crimea (2014), and its
                interference in democratic processes in Western countries (2016 onwards), as well as the
                proliferation of hostile influence campaigns across the world, a great number of studies
                (policy and academic) focused on the analysis, detection, and characterization of
                disinformation online, with a wide-ranging topical diversity ranging from the detection of
                computational propaganda to the use of automated accounts, narratives, coordinated
                information maneuvers, platform architectures, root causes, susceptibility of audiences, and
                to strategic/tactical characterization of hostile actors. Overall, the mental models and
                conceptual frameworks varied across thousands of studies.
            </p>
            <p>
                In recent years, some frameworks have been proposed (and partially adopted) to improve the
                characterization of influence operations across agencies for more effective documentation,
                understanding, and mitigation of threat incidents. The ABCDE Framework is among the most
                recent frameworks that achieved significant adoption across different government entities,
                especially in the European Union. The framework is based on five primary analytical
                components for the characterization of detected influence operations: Actor, Behavior,
                Content, Degree, and Effect. According to the original report, each component in the ABCDE
                framework can be formalized as a set of questions (see figure 1).
            </p>
            <img src="assets/images/abcdediagram.png" alt=”abcde-diagram”
            width="80%">
            <br>
            <b>Figure 1.</b> ABCDE Framework (source: Carnegie Endowment for International Peace).
            
            <p>
                <br>
                More recently, the SCOTCH framework has been proposed in an Atlantic Council publication as
                a result of a series of expert consultations across institutions. The SCOTCH Framework aims
                to enable inter-institutional and structured communication of the findings regarding the
                detected/assessed incidents of hostile influence operations and disinformation across online
                platforms. Namely, the components of the framework are the Source, Channel, Objective,
                Target, Composition, and Hook. Overall, the elements of the SCOTCH Framework can be mapped
                to the structure of the ABCDE Framework, as the two frameworks have some similarities in
                their original objectives. However, the SCOTCH framework has a greater emphasis on the
                platform architectures, actor motivations (original intent), strategic objectives, and
                targets of the influence operations. Moreover, the SCOTCH Framework also emphasizes the
                "Bottom Line Up Front" assessments, similar to the long-lasting intelligence analysis best
                practices, mainly for quick and effective communication.
            </p>
            <p>
                Before the ABCDE and SCOTCH frameworks, several other proposals existed. For example, in a
                joint proposal by Graphika and Harvard University, "Disinformation ABC" aimed at
                characterizing online disinformation across three vectors: "A" for manipulative actors, "B"
                for deceptive behavior, and "C" for harmful content. Later, a different proposal added the
                "D" vector to the ABC, to address the need for understanding "information distribution"
                dynamics in influence operations. The Information Disorder framework, as a broader
                conceptual framework for understanding the information incidents and campaigns, included
                both object types and phases of any information event. For object types, the information
                disorder framework includes the Agent, Message, and Interpreter elements, while each
                campaign/incident is characterized by considering the "Creation", "Reproduction", and
                "Distribution" phases.
            </p>
            <p>
                Also, other frameworks had application-specific focus. For example, "the Department of
                Justice (DoJ) Framework to Counter Malign Foreign Influence Operations" intuitively had a
                legal perspective on hostile influence. Conceptually, the DoJ framework also introduced a
                combined articulation of cybersecurity threats and malign influence campaigns. The major
                categories in the DoJ framework are "cyber operations targeting election infrastructure,
                cyber operations targeting political parties, campaigns, and public officials, covert
                influence operations to assist or harm political organizations, campaigns and public
                officials, covert influence operations to influence public opinion and sow division, and
                overt influence efforts to influence policymakers and the public".
            </p>
        </div>
    </div>
</div>

<!--Framework focusing on process, ttps, behavior-->
<div class="collapsible" id="framework-for-process">
    <button type="button" class="collapsible_button">
        <h6>Frameworks Focusing on Process, TTPs, Behaviors
        </h6>
    </button>

    <div class="collapsible-content">
        <p>
            The second major category of analytical frameworks focuses on the processes that often include
            multiple phases, and associated behaviors, tactics, techniques, and procedures (TTPs). More
            often than not, frameworks in this category rely on two types of conceptualizations. In the
            first group, the influence campaigns are assessed within a framework similar to pre-existing
            best practices of cybersecurity, resembling the "cyber kill chains". The second group of
            frameworks relies on the categorization of narrative-related, rhetorical, social, and
            network-centric maneuvers in the information domain.
        </p>
        <p>
            The concept of "social media kill chains" was proposed as a structured process to assess hostile
            influence, based on the previously existed workflows of the cyber kill chain. The social media
            kill chain concept includes staging, reconnaissance, targeting, mimicry, content placement,
            amplification, and mobilization. In the given conceptual framework, each phase has a different
            set of relevant analytical questions and potential tactical components. In a relevant framework
            articulated in a report by the Department of Homeland Security (DHS), "Disinformation Kill
            Chain" included Recon, Build, Seed, Copy, Amplify, Control, and Effect phases (see figure 2).
            The DHS framework mapped phases of the disinformation kill chain with corresponding response
            themes, drivers, and potential mitigation efforts.
        </p>
        <div>
            <img src="assets/images/disinformationkilldiagram.png" alt=disinformationkilldiagram
                width="80%">
            <br>
            <b>Figure 2.</b> Disinformation Kill Chain (source: MITRE Corp.).
        </div>
        <p>
            <br>
            Within the same family of cybersecurity inspired frameworks, the DISARM Framework, which
            originally started with the name of AMITT (Adversarial Misinformation and Influence Tactics and
            Techniques), maps the influence operation TTPs across different phases and strategic components.
            The DISARM/AMITT was built on the MITRE ATT&CK framework for cybersecurity. The four phases of
            hostile influence campaigns in the DISARM include the planning, preparation, execution, and
            evaluation, while the overall operational categories range from strategic planning to objective
            planning, people developing, network developing, microtargeting, content developing, channel
            selection, pump-priming, exposure, physical aspects, persistence, and effectiveness measuring.
            Each operational component of the DISARM has a number of documented influence operation
            techniques than can be used to characterize an ongoing or previous incident/campaign (see figure
            3).
        </p>
        <p>
            Another recent framework, RICHDATA, has been proposed by a research group at the Center for
            Security and Emerging Technology. The RICHDATA framework mostly resembles the DISARM/AMITT, with
            phases including Reconnaissance, Infrastructure, Content Creation & Hijacking, Deployment,
            Amplification, Troll Patrol, and Actualization.
        </p>
        <p>
            As mentioned above, the second type of frameworks in this category emphasizes the
            narrative-related, behavioral, and network-centric "maneuvers" exhibited in online influence
            operations. In one of the earliest and most widely adopted examples, the "4Ds of disinformation"
            behaviors were outlined as "dismiss", "distort", "distract", and "dismay", often adopted as a
            conceptual framework to characterize the Russian disinformation campaigns. Other more recent
            frameworks built on the "4Ds" model, adding new behavioral and narrative-related components to
            the characterization of influence operations. The BEND Framework, for example, maps the relevant
            behaviors across 16 different behaviors, categorized as information and network maneuvers.

        </p>
        <p>
            Accordingly, the BEND behaviors include maneuvers that manipulate information/narratives, or
            social/communication networks.
        </p>
        <div>
            <img src="assets/images/disarm-diagram.png" alt=disarm-diagram”
                width="80%">
            <br>
            <b> Figure 3.</b> DISARM/AMITT Framework.
        </div>
        <p>
            <br>
            Finally, as an example of narrative-related, and rhetorical frameworks of influence in general,
            the "Playmaker Taxonomy of Influence Strategies" maps the rhetorical and social maneuvers of
            influence. While the taxonomy was used to analyze the Russian Federation's influence strategies
            in a recent report by the NATO Strategic Communications Centre of Excellence, the given
            framework is not specific to hostile influence campaigns online.
        </p>
    </div>
</div>
